{
    "build": {
        "dockerfile": "Dockerfile",
        "context": ".."
    },
    "runArgs": [
        "--gpus=all",
        "--network",
        "ollama_network",
        "--name",
        "devcontainer_ollama"
    ],
    "postStartCommand": "ollama serve & sleep 5 && ollama run llama3.1",
    "customizations": {
        "vscode": {
            "settings": {
                "remote.autoForwardPorts": false
            },
            "extensions": [
                "rangav.vscode-thunder-client"
            ]
        }
    }
}