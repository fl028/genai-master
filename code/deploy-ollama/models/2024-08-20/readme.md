# llama3.1-finetuned-2024-08-20
- 1k training rows
- updated params (r=32,lora_alpha=32,lora_dropout=0.05,per_device_train_batch_size=8,gradient_accumulation_steps=4,learning_rate=3e-5)
- updated summary creation (promts)